{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c545966e",
   "metadata": {},
   "source": [
    "We want to do a book recommendation system based on the data we have cleaned before.\n",
    "They are three types of recommender system:\n",
    "* Collaborative filtering:This system matches persons with similar interests and provides recommendations based on this matching. It needs the historical activity of the users which we have.It can be user-based or item-based. In the first, we recommend items to a user that similar users have also liked. In the second, we recommend items based on the past ratings of similiar items by the user.\n",
    "\n",
    "* Content base systems: They suggest similar items based on a particular item. This system uses item metadata. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it. The only metadata we have about the books in our dataset is the title, author and country wich is very little. A good metadata is the genres or the summary that sadly we haven't. This method is also computationally expensive which is another reason we are not doing it.\n",
    "\n",
    "This system recommendation were mainly done with the helf of the book \"Hands on recommendation systems with Python\" by Rounak Banik \n",
    "https://learning.oreilly.com/library/view/hands-on-recommendation-systems/9781788993753/5f1269b2-2007-42a8-a900-bf255b86e64c.xhtml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a94ed",
   "metadata": {},
   "source": [
    "We want to predict the rating given by a user to a book based on the book that he has already read and the books of user that have like the same books i.e similar users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8f753",
   "metadata": {},
   "source": [
    "# Loading the data previously cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbc6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2dded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:/Workspace_Python/MLProject/MachineLearningDSTIProject/dataset/\"\n",
    "file=\"final_dataset.csv\"\n",
    "users_csv=\"cleaned_users.csv\"\n",
    "books_csv=\"cleaned_books.csv\"\n",
    "ratings_csv=\"cleaned_ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca42950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+file, sep=\";\",on_bad_lines='warn', encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9290332",
   "metadata": {},
   "source": [
    "# Recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4702d3",
   "metadata": {},
   "source": [
    "## Item-based ou user-based collaborative filtering ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f8f446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278858, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(path+users_csv, sep=\";\",on_bad_lines='warn', encoding=\"latin-1\")\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9db7fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248251"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = pd.read_csv(path+books_csv, sep=\";\",on_bad_lines='warn', encoding=\"latin-1\")\n",
    "books.bookId.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6ed55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>bookAuthor</th>\n",
       "      <th>yearOfPublication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>bookId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ISBN, bookTitle, bookAuthor, yearOfPublication, publisher, bookId]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.loc[books.bookId==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96a7a9",
   "metadata": {},
   "source": [
    "As we have more users than books and we have huge disparity in the number of ratings between users as seen in the EDA, we choose to use the item-based collaborative filtering which should also be more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56beb2d9",
   "metadata": {},
   "source": [
    "## Item-based collaborative filtering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43600547",
   "metadata": {},
   "source": [
    "We use the library surprise to do the recommender system. It's a library specialised in this kind of machine learning. Sadly we could not use the age with this library as only 4 columns are allowed: user, item, rating and timestamp. The age could have been useful to better determine the similitude between two users or items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b2563",
   "metadata": {},
   "source": [
    "We load the data into the data structure of the surprise library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8f6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Pandas dataframe\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "#we reduce the size of the dataset by removing the user that rated less than 10 books and books \n",
    "#that have been rated less than 100 times\n",
    "df=df[df.userID.isin(df.userID.value_counts()[df.userID.value_counts()>10].index)][['userID','bookId','bookRating']]\n",
    "df=df[df.bookId.isin(df.bookId.value_counts()[df.bookId.value_counts()>100].index)][['userID','bookId','bookRating']]\n",
    "data = Dataset.load_from_df(df[['userID','bookId','bookRating']], Reader(line_format='user item rating', sep=\";\",rating_scale=(1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d8afab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242</td>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243</td>\n",
       "      <td>305</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243</td>\n",
       "      <td>822</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243</td>\n",
       "      <td>512</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>243</td>\n",
       "      <td>4675</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating timestamp\n",
       "0      242      180     8.0      None\n",
       "1      243      305     7.0      None\n",
       "2      243      822     9.0      None\n",
       "3      243      512    10.0      None\n",
       "4      243     4675     9.0      None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##to see what is in the built in data of surprise\n",
    "d=pd.DataFrame(data.__dict__['raw_ratings'], columns=['user_id','item_id','rating','timestamp'])\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bfdcd1",
   "metadata": {},
   "source": [
    "We benchmark the different models by using a cross validation after having set the the seed of the RNG to have reproducible experiments. We use RMSE to measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff64d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "my_seed = 42\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be2acac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>1.626885</td>\n",
       "      <td>0.014229</td>\n",
       "      <td>0.009009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>1.634637</td>\n",
       "      <td>0.104982</td>\n",
       "      <td>0.013174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>1.654903</td>\n",
       "      <td>0.235163</td>\n",
       "      <td>0.260968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVDpp</th>\n",
       "      <td>1.680577</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.036880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBasic</th>\n",
       "      <td>1.737566</td>\n",
       "      <td>0.210171</td>\n",
       "      <td>0.196236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithMeans</th>\n",
       "      <td>1.830618</td>\n",
       "      <td>0.266791</td>\n",
       "      <td>0.209236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SlopeOne</th>\n",
       "      <td>1.843097</td>\n",
       "      <td>0.051604</td>\n",
       "      <td>0.018613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoClustering</th>\n",
       "      <td>1.851605</td>\n",
       "      <td>0.489725</td>\n",
       "      <td>0.009844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>1.875192</td>\n",
       "      <td>0.404463</td>\n",
       "      <td>0.221646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NormalPredictor</th>\n",
       "      <td>2.447643</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>0.009405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF</th>\n",
       "      <td>2.760879</td>\n",
       "      <td>0.363627</td>\n",
       "      <td>0.014932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_rmse  fit_time  test_time\n",
       "Algorithm                                      \n",
       "BaselineOnly      1.626885  0.014229   0.009009\n",
       "SVD               1.634637  0.104982   0.013174\n",
       "KNNBaseline       1.654903  0.235163   0.260968\n",
       "SVDpp             1.680577  0.127168   0.036880\n",
       "KNNBasic          1.737566  0.210171   0.196236\n",
       "KNNWithMeans      1.830618  0.266791   0.209236\n",
       "SlopeOne          1.843097  0.051604   0.018613\n",
       "CoClustering      1.851605  0.489725   0.009844\n",
       "KNNWithZScore     1.875192  0.404463   0.221646\n",
       "NormalPredictor   2.447643  0.009133   0.009405\n",
       "NMF               2.760879  0.363627   0.014932"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import SlopeOne\n",
    "from surprise import NMF\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBaseline\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import BaselineOnly\n",
    "from surprise import CoClustering\n",
    "from surprise.model_selection import cross_validate\n",
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(verbose =False), SVDpp(verbose =False), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(verbose =False), KNNBasic(verbose =False), KNNWithMeans(verbose =False), KNNWithZScore(verbose =False), BaselineOnly(verbose =False), CoClustering(verbose =False)]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=5, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = pd.concat([tmp,pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm'])])\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34537c50",
   "metadata": {},
   "source": [
    "BaselineOnly and SVD give us the best although BaselineOnly also give us the best time, so it' is one of this two we should use. As KNNBseline can be configured by using similitude we also futher test it with both user_based and item_based configuration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee501b",
   "metadata": {},
   "source": [
    "## Hyper_tuning of BaseLineOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2affe701",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5972535959837304\n",
      "{'bsl_options': {'method': 'als', 'n_epochs': 25, 'reg_u': 4, 'reg_i': 5}}\n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "bsl_options = {\"method\": [\"als\"], \"n_epochs\": [10,15,20,25 ], \"reg_u\": [4,5,10,15,20], \"reg_i\": [5,10,15,20]}\n",
    "param_grid={\"bsl_options\":bsl_options}\n",
    "gs = GridSearchCV(BaselineOnly, param_grid, measures=[\"rmse\", \"mae\"], cv=5,n_jobs =2, joblib_verbose=0)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9879a0",
   "metadata": {},
   "source": [
    "## Hyper tuning of SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c06818c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6153179196047005\n",
      "{'n_epochs': 20, 'lr_all': 0.02, 'reg_all': 0.02}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_epochs\": [15,20,25,30],\"lr_all\": [0.004,0.008,0.01,0.02],\"reg_all\": [0,0.02,0.4,0.5, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=5,n_jobs =2, joblib_verbose=0)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b31271",
   "metadata": {},
   "source": [
    "## Hyper tuning of KNNBaseLine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b67d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6303375645029043\n",
      "{'bsl_options': {'method': 'als', 'n_epochs': 25, 'reg_u': 2, 'reg_i': 4}, 'sim_options': {'name': 'cosine', 'user_based': True}, 'k': 60}\n"
     ]
    }
   ],
   "source": [
    "bsl_options = {\"method\": [\"als\"], \"n_epochs\": [15,20,25 ], \"reg_u\": [2,5,8,10], \"reg_i\": [2,4,6]}\n",
    "sim_options = {\"name\": [\"cosine\"],\"user_based\": [True,False]}\n",
    "param_grid = {\"bsl_options\":bsl_options, \"sim_options\":sim_options, \"k\":[20,40,60]}\n",
    "gs = GridSearchCV(KNNBaseline, param_grid, measures=[\"rmse\", \"mae\"], cv=5,n_jobs =2, joblib_verbose=0)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdea681",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f6074",
   "metadata": {},
   "source": [
    "The benchmark and the tunning of the model show that the best model is the BaseLineOnly with the Alternating Least Squares procedure configured with 25 iterations,reg_u=4 and reg_i=5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79da9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "bsl_options= {'method': 'als', 'n_epochs': 25, 'reg_u': 4, 'reg_i': 5}\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "algo = BaselineOnly(bsl_options=bsl_options,verbose =False)\n",
    "predictions = algo.fit(trainset).test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53f9e8",
   "metadata": {},
   "source": [
    "Finally, we measure the accuracy of the ratings we have get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7885730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5899\n",
      "MSE: 2.5278\n",
      "MAE:  1.2344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.234437785004282"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.rmse(predictions)\n",
    "accuracy.mse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2b746",
   "metadata": {},
   "source": [
    "As a good recommender system should have RMSE under 1, we could say that our recommender system is average as the metrics aren't too far of the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab42e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a63766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "precision =  0.9673444976076555\n",
      "recall =  0.9714247740563531\n",
      "--------------------------------------\n",
      "precision =  0.968816348195329\n",
      "recall =  0.9703802699423718\n",
      "--------------------------------------\n",
      "precision =  0.9724203133441384\n",
      "recall =  0.9747451360093174\n",
      "--------------------------------------\n",
      "precision =  0.9729950900163666\n",
      "recall =  0.9744271685761048\n",
      "--------------------------------------\n",
      "precision =  0.9676317162232654\n",
      "recall =  0.970016394664282\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "bsl_options= {'method': 'als', 'n_epochs': 25, 'reg_u': 4, 'reg_i': 5}\n",
    "algo = BaselineOnly(bsl_options=bsl_options,verbose =False)\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "    print(\"--------------------------------------\")\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(\"precision = \",sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(\"recall = \",sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42a69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c669c4f8",
   "metadata": {},
   "source": [
    "We can now do some prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3a1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from neo4j import GraphDatabase\n",
    "#driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"DSTI2023!!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
